{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Predicción de tendencia del dolar\n",
        "\n",
        "Este Jupyter Notebook tiene como objetivo desarrollar y evaluar modelos predictivos para la tendencia del dólar utilizando dos algoritmos de aprendizaje automático: Random Forest y Gradient Boosting. Para ello, se implementarán dos pipelines de procesamiento y modelado distintos, cada uno asociado a uno de estos algoritmos. El análisis se realizará sobre un conjunto de datos históricos de las tasas de cambio del dólar, incluyendo diversas variables que podrían influir en su comportamiento."
      ],
      "metadata": {
        "id": "dnglvDsxGUs2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalamos paquetes"
      ],
      "metadata": {
        "id": "NLp1XZufHU10"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "H7Gg7yg9Fjzb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import f1_score, ConfusionMatrixDisplay, confusion_matrix, mean_absolute_error\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import (\n",
        "    MinMaxScaler,\n",
        "    OneHotEncoder,\n",
        "    OrdinalEncoder,\n",
        "    StandardScaler,\n",
        "    PolynomialFeatures,\n",
        "    FunctionTransformer,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hacemos esas cosas genéricas que el profe siempre hace en clase"
      ],
      "metadata": {
        "id": "uM42a8vYHeF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definamos el \"random_state\" para que los resultados sean reproducibles:\n",
        "random_state=42\n",
        "# Cambiemos la fuente de las gráficas de matplotlib:\n",
        "plt.rc('font', family='serif', size=12)"
      ],
      "metadata": {
        "id": "M1B6JRhdHkOv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descargamos los datos\n",
        "\n",
        "Nos traemos el csv y creamos el dataset. El head() lo estamos usando para asegurarnos que tenemos los datos que son."
      ],
      "metadata": {
        "id": "sRs2adhUJAyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/spreadsheets/d/1N90hH0WpO7IhK5ptAW-kWakol71F8GfkiFp0bYlyMVI/gviz/tq?tqx=out:csv&sheet=ML-Dolar%20(Full%20Dataset)' -O dollar_dataset.csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDSTp0oZRMC1",
        "outputId": "1d3853d5-6e8b-459e-c921-35baf7355f0a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-23 17:46:21--  https://docs.google.com/spreadsheets/d/1N90hH0WpO7IhK5ptAW-kWakol71F8GfkiFp0bYlyMVI/gviz/tq?tqx=out:csv&sheet=ML-Dolar%20(Full%20Dataset)\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.101.138, 142.250.101.100, 142.250.101.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.101.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/csv]\n",
            "Saving to: ‘dollar_dataset.csv’\n",
            "\n",
            "dollar_dataset.csv      [ <=>                ]  66.65K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2023-10-23 17:46:21 (13.0 MB/s) - ‘dollar_dataset.csv’ saved [68249]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar los datos\n",
        "dollar_df = pd.read_csv(\"dollar_dataset.csv\", index_col=0)\n",
        "dollar_df.dtypes\n",
        "\n",
        "# Este drop elimina las columnas no influyentes. No lo hacemos en este notebook porque queremos gráficos que nos permitan conclusiones\n",
        "# dollar_df.drop(columns=['inflacion_eu', 'pib_pesos', 'deuda_ext_total', 'inflacion_usa', 'deuda_ext_publica'], inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InXr9TbxTKsI",
        "outputId": "4a5c2261-98ac-4158-a48b-497bc75f1f6b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "inflacion_col                    float64\n",
              "inflacion_usa                    float64\n",
              "inflacion_eu                     float64\n",
              "deuda_ext_publica                float64\n",
              "deuda_ext_privada                float64\n",
              "deuda_ext_total                  float64\n",
              "pib_pesos                        float64\n",
              "prime_rate                       float64\n",
              "cafe_miles_dolares               float64\n",
              "cafe_toneladas_cubicas           float64\n",
              "carbon_miles_dolares             float64\n",
              "carbon_toneladas_cubicas         float64\n",
              "petroleo_miles_dolares           float64\n",
              "petroleo_toneladas_cubicas       float64\n",
              "ferroniquel_miles_dolares        float64\n",
              "ferroniquel_toneladas_cubicas    float64\n",
              "trm                              float64\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vamos a partir el dataset en datos de entrenamiento y datos de prueba\n",
        "\n",
        "Como no tenemos suficientes datos, no vamos a trabajar con datos de validación"
      ],
      "metadata": {
        "id": "ESE9Qz9wasY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    dollar_df.drop(columns='trm'),\n",
        "    dollar_df['trm'],\n",
        "    test_size = 0.2,\n",
        "    random_state = random_state\n",
        ")"
      ],
      "metadata": {
        "id": "4O55LVM_aznW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nos embarcamos en la travesía de quitar los nulos y escalar los datos\n",
        "\n",
        "Debido a que los datos que tenemos son menusales, pero temas de inflación y deuda externa los tenemos anuales, debemos normalizar esas columnas para que tengamos datos mensuales que puedan contribuir a la predicción.\n",
        "\n",
        "Vamos a crear el pipeline que necesitamos."
      ],
      "metadata": {
        "id": "jyf2JtfHYmnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dollar_df.hist(bins=50, figsize=(20, 15));"
      ],
      "metadata": {
        "id": "uH7ubaVHhLkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZEIo2ZYAhKld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.describe()\n"
      ],
      "metadata": {
        "id": "P9l42KEtVJnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Listamos la cantidad de columnas con cantidad de nulos\n",
        "X_train.isna().sum()"
      ],
      "metadata": {
        "id": "6R7qnKp1X75A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vamos a arrancar nuestro proceso para entrenar el modelo"
      ],
      "metadata": {
        "id": "-hQW0f6Kby7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encontrar variables numéricas y categóricas\n",
        "num_cols = X_train.select_dtypes(include=np.number).columns\n",
        "\n",
        "# Definir el pipeline de pre-procesamiento\n",
        "numeric_transformer = Pipeline(\n",
        "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")),(\"scaler\", StandardScaler())]\n",
        ")\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers = [\n",
        "       ('num',numeric_transformer,num_cols)\n",
        "      ])\n",
        "\n",
        "#Definimos nuestro regresor\n",
        "rf_base = RandomForestRegressor(random_state=random_state)\n",
        "gb_base = GradientBoostingRegressor(random_state=random_state)\n",
        "\n",
        "#Definimos los pipelines\n",
        "pipeline_rf = Pipeline(steps = [\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', rf_base),\n",
        "                            ])\n",
        "pipeline_gb = Pipeline(steps = [\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', gb_base),\n",
        "    ])\n",
        "#{'regressor__max_depth': 9, 'regressor__min_samples_leaf': 10, 'regressor__n_estimators': 100}\n",
        "# Definamos la malla de parámetros sobre la que haremos la búsqueda:\n",
        "param_grid = {\n",
        "    'regressor__n_estimators': [50, 100],\n",
        "    'regressor__max_depth': [9],\n",
        "    'regressor__min_samples_leaf': [10, 300, 2000]\n",
        "}\n",
        "\n",
        "# Definamos nuestros modelo mediante GridSearchCV:\n",
        "rf = GridSearchCV(pipeline_rf, cv=3, param_grid=param_grid)\n",
        "gb = GridSearchCV(pipeline_gb, cv=3, param_grid=param_grid)"
      ],
      "metadata": {
        "id": "Vg6Zieqib3v0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Entrenemos el random forest:\n",
        "rf.fit(X_train, y_train)\n",
        "# Entrenemos el gradient boosting:\n",
        "gb.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "EJPnLo6Fd6Kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rf.best_params_)\n",
        "print(gb.best_params_)\n"
      ],
      "metadata": {
        "id": "2g0CHyhRd8XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtengamos el R^2 y el MAE de entrenamiento para cada modelo:\n",
        "print(\"Train set\")\n",
        "for model in (rf, gb):\n",
        "    print(f\"Model: {'Random Forest' if model == rf else 'Gradient Boosting'}\")\n",
        "    print(f'R^2: {model.score(X_train, y_train)}')\n",
        "    print(f'MAE: {mean_absolute_error(y_train, model.predict(X_train))}')\n",
        "    print('\\n')\n",
        "\n",
        "print(\"Test set\")\n",
        "for model in (rf, gb):\n",
        "    print(f\"Model: {'Random Forest' if model == rf else 'Gradient Boosting'}\")\n",
        "    print(f'R^2: {model.score(X_test, y_test)}')\n",
        "    print(f'MAE: {mean_absolute_error(y_test, model.predict(X_test))}')\n",
        "    print('\\n')\n"
      ],
      "metadata": {
        "id": "y2cTwXOId_xZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Veamos los datos junto con las predicciones:\n",
        "pd.concat([\n",
        "    dollar_df.drop(columns='trm').reset_index(drop=True),\n",
        "    dollar_df['trm'].reset_index(drop=True),\n",
        "    pd.DataFrame({'rf_predicted_trm': rf.predict(dollar_df.drop(columns=['trm']))}),\n",
        "    pd.DataFrame({'gb_predicted_trm': gb.predict(dollar_df.drop(columns=['trm']))})\n",
        "], axis=1)\n"
      ],
      "metadata": {
        "id": "IFi4lkX5e2Pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explicabilidad"
      ],
      "metadata": {
        "id": "-CMCyOEW4_5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap"
      ],
      "metadata": {
        "id": "A1iMJH4b5CZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "from IPython.display import HTML"
      ],
      "metadata": {
        "id": "1uPabbmC5EZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamos nuestro pipeline con los mejores parámetros encontrados en la validación cruzada\n",
        "pipeline_gb.set_params(**gb.best_params_)\n",
        "pipeline_gb.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "S8roZ_rP5XWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_gb.named_steps.keys()"
      ],
      "metadata": {
        "id": "yJNWdtuU5Zy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Aplicamos el preproceso a los datos\n",
        "X_train_preprocessed = pipeline_gb.named_steps[\"preprocessor\"].transform(X_train)\n",
        "X_test_preprocessed = pipeline_gb.named_steps[\"preprocessor\"].transform(X_test)\n"
      ],
      "metadata": {
        "id": "Bh1MgH8X5cxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenemos nuestro modelo de explicabilidad\n",
        "gb_explainer = shap.TreeExplainer(pipeline_gb.named_steps[\"regressor\"])\n",
        "train_gb_shap_values = gb_explainer.shap_values(X_train_preprocessed)\n",
        "test_gb_shap_values = gb_explainer.shap_values(X_test_preprocessed)\n"
      ],
      "metadata": {
        "id": "kYKkPwVb5kIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary plot\n",
        "shap.summary_plot(test_gb_shap_values, X_test, plot_type=\"bar\")\n"
      ],
      "metadata": {
        "id": "Uuj7NXQ65ufA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grafiquemos los valores SHAP para un ejemplo del conjunto de test\n",
        "shap.initjs()\n",
        "\n",
        "instance_index = 0\n",
        "force_plot_html = shap.force_plot(gb_explainer.expected_value, test_gb_shap_values[instance_index], X_test.iloc[instance_index],show=False)\n",
        "HTML(force_plot_html.html())\n"
      ],
      "metadata": {
        "id": "irLl-LRT5w1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grafiquemos la dependencia de los valores SHAP con la variable \"prime_rate\"\n",
        "shap.dependence_plot('prime_rate', test_gb_shap_values, X_test)\n"
      ],
      "metadata": {
        "id": "7jHagWAp507L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}